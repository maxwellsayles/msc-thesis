\documentclass[11pt, letterpaper]{article}
\usepackage{geometry}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{url}
\usepackage{fullpage}
\usepackage{verbatim}

\usepackage{theorem}
\theorembodyfont{\rmfamily}

\newenvironment{definition}
{\noindent\begin{defn}}
{\hfill $\Box$ \end{defn}}
\newtheorem{defn}{Definition}

\newenvironment{theorem}
{\noindent\begin{thm}}
{\hfill $\Box$ \end{thm}}
\newtheorem{thm}{Theorem}

\newenvironment{corollary}
{\noindent\begin{cor}}
{\hfill $\Box$ \end{cor}}
\newtheorem{cor}{Corollary}

\newenvironment{lemma}
{\noindent\begin{lem}}
{\hfill $\Box$ \end{lem}}
\newtheorem{lem}{Lemma}

\newenvironment{proposition}
{\noindent\begin{prop}}
{\hfill $\Box$ \end{prop}}
\newtheorem{prop}{Proposition}


%\setlength\topmargin{0in}
%\setlength\headsep{0.25in}
%\setlength\headheight{0in}
%\setlength\textheight{8.5in}
%\setlength\textwidth{6.0in}
%\setlength\oddsidemargin{0.25in}
%\setlength\evensidemargin{0in}

\parindent 0ex

\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\matrixot}[2]{\left( \begin{array}{c} #1 \\ #2 \end{array} \right)}
\newcommand{\matrixtt}[4]{\left( \begin{array}{cc} #1 & #2 \\ #3 & #4 \end{array} \right)}
\newcommand{\ntoinfty}{\lim_{n \rightarrow \infty}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}



\begin{document}

\title{Summary of definitions and theorems\\for Chapter 11 from Snell 2006}
\maketitle


Let the $m \times m$ matrix $P$ denote our transition matrix.  Then $P^{(n)}_{ij}$ denotes the probability of being in state $s_j$ if we start in state $s_i$ after $n$ moves. The matrix $P$ can also be thought of as an adjacency matrix for a directed graph. Then, concatenation with $P$ denotes the probability of walking from $s_i$ to $s_j$ on the graph. \\

\begin{definition} A state $s_i$ is \textit{absorbing} if $p_{ii} = 1$.  A chain is \textit{absorbing} if it has an absorbing state, and it is possible to get there from all other states.
\end{definition}

\begin{definition} If a chain is absorbing, a state which is not absorbing is called \textit{transient}.
\end{definition}

\begin{definition} Renumber the states so that the transient states come first.  The \textit{canonical form} of $P$ is given by:
\[
	P = \begin{array}{l l}
		& \begin{array}{l l}\textrm{TR.} & \textrm{ABS.}\end{array} \\
		\begin{array}{l}\textrm{TR.} \\ \textrm{ABS.}\end{array} & 
		\left( \begin{array}{c | c} Q & R \\ \hline 0 & I \end{array} \right)
	    \end{array}.
\]
Since $Q$ consists of all the transient states, $Q^n \rightarrow 0$.
\end{definition}

\begin{theorem}
For an absorbing chain, let $N = (I-Q)^{-1} = I + Q + Q^2 + \cdots$.  Then $n_{ij}$ is the expected number of times the chain is in state $s_j$ if it starts in $s_i$.  This is known as the \textit{fundamental matrix} for $P$.
\end{theorem}

\begin{theorem}
Let $t = Nc$ where $c$ is a column vector of all 1.  Then $t_i$ is the time to absorption starting in start $s_i$.
\end{theorem}

\begin{theorem}
Let $B = NR$.  Then $B_{ij}$ is the probability of being absorbed into state $s_j$ when starting in the transient state $s_i$.
\end{theorem}

\begin{definition}
A chain is \textit{ergodic} if it is possible to go from every state to every state, i.e. the chain represents a strongly connected graph.
\end{definition}

\begin{definition}
A chain is \textit{regular} if some power of $P$ has all positive non-zero elements, i.e. if there is some value $n$ for which it is possible to walk from any state to any state in exactly $n$ moves.
\end{definition}

\begin{theorem}
If $P$ is a regular matrix, then $P^n \rightarrow W$ with all rows the same vector $w$ and $w$ is strictly positive.  This implies that $WP = W$, that $wP = w$, and for any $vP=v$ then $v$ is a multiple of $w$.  Furthermore, $Pc = c$ where $c$ is a column vector of all ones, and for any $x$ such that $Px = x$ then $x$ is a multiple of $c$. The \textit{fixed row vector} is $w$ and the \textit{fixed column vector} is $c$. It follows that for any initial probability vector $u$, $uW = w$, i.e. it doesn't matter what the starting probability is in the long term.
\end{theorem}

\begin{corollary}
Let $W$ be the matrix of fixed row vectors. Then $W^k = W$ for all $k \in \NN$.
\end{corollary}

\begin{theorem}
The fundamental matrix for an ergodic chain is denoted by
\[
	Z = (I-P+W)^{-1}.
\]
\end{theorem}



\begin{theorem}
If $P$ is an ergodic transition matrix, then $\overline{P} = (1/2)I + (1/2)P$ is a regular chain with the same fixed vectors.  Any ergodic transition matrix can be made regular by adding any positive weighted value of the identity matrix.
\end{theorem}

\begin{definition}
The \textit{mean first passage time}, denoted by $m_{ij}$, is the expected number of steps to reach $s_j$ for the first time if started in $s_i$.
\end{definition}

\begin{theorem}
Let $Z = (I-P+W)^{-1}$.  The mean first passage time can then be calculated with
\[
	m_{ij} = \frac{z_{jj} - z_{ij}}{w_j}.
\]

\end{theorem}

\begin{definition}
The \textit{mean recurrence time}, denoted $r_i$, is the expected number of steps to return to $s_i$ for the first time.
\end{definition}

\begin{theorem}
The mean recurrence time for $s_i$ is given by $r_i = 1/w_i$.
\end{theorem}

\begin{theorem}
An ergodic chain is reversible if and only if for every pair of states $s_i$ and $s_j$, \\
$w_i p_{ij} = w_j p_{ji}$.  Let $P^*$ denote the reversible transition matrix, then
\[
	p^*_{ij} = \frac{w_j p_{ji}}{w_i}.
\]
\end{theorem}


\end{document}

