\documentclass[11pt, letterpaper]{article}
\usepackage{geometry}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{url}
\usepackage{fullpage}
\usepackage{verbatim}

\usepackage{theorem}
\theorembodyfont{\rmfamily}

\newenvironment{definition}
{\noindent\begin{defn}}
{\hfill $\Box$ \end{defn}}
\newtheorem{defn}{Definition}

\newenvironment{theorem}
{\noindent\begin{thm}}
{\hfill $\Box$ \end{thm}}
\newtheorem{thm}{Theorem}

\newenvironment{corollary}
{\noindent\begin{cor}}
{\hfill $\Box$ \end{cor}}
\newtheorem{cor}{Corollary}

\newenvironment{lemma}
{\noindent\begin{lem}}
{\hfill $\Box$ \end{lem}}
\newtheorem{lem}{Lemma}

\newenvironment{proposition}
{\noindent\begin{prop}}
{\hfill $\Box$ \end{prop}}
\newtheorem{prop}{Proposition}



\parindent 0ex

\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\matrixot}[2]{\left( \begin{array}{c} #1 \\ #2 \end{array} \right)}
\newcommand{\matrixtt}[4]{\left( \begin{array}{cc} #1 & #2 \\ #3 & #4 \end{array} \right)}
\newcommand{\ntoinfty}{\lim_{n \rightarrow \infty}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}



\begin{document}

\title{Probability \& Statistics}
\maketitle

\begin{definition}
A \textit{random variable} resembles a function that maps the sample space of a random process to the real numbers.
\end{definition}

\begin{definition}
An \textit{event} is a subset of the sample space, i.e. a subset of the possible outcomes of a random variable.
\end{definition}

\begin{definition}
A \textit{probability mass function} gives the probability that a discrete random variable is exactly equal to some value.  In contrast, for continuous random variables, the integral over some area of a \textit{probability density function} gives the probability that the continuous random variable will fall within that range.
\end{definition}



\begin{theorem}
The probability of $B$ given $A$ is defined as
\[
	P(B|A) = \frac{P(B \cap A)}{P(A)}.
\]
\end{theorem}

\begin{definition}
Two events, $A$ and $B$, are said to be \textit{independent} if
\[
	P(A \cap B) = P(A) \cdot P(B).
\]
\end{definition}

\begin{definition}
Two events, $A$ and $B$ are said to be \textit{mutually exclusive} or \textit{disjoint} if
\[
	P(A \cap B) = \emptyset
\]
and if they are disjoint, then
\[
	P(A \cup B) = P(A) + P(B).
\]
\end{definition}

\begin{definition}
The event $A \cap B$ is referred to as $A$ \textit{and} $B$, and the event $A \cup B$ as $A$ \textit{or} $B$.
\end{definition}

\begin{definition}
If $X$ is a discrete random variable with probability mass function $P(x)$, then the \textit{expected value} is defined as
\[
	E(X) = \sum_i x_i \cdot P(x_i).
\]
If the probability distribution of $X$ admits a probability density function $f(x)$, then the \textit{expected value} can be computed as
\[
	E(X) = \int_{-\infty}^{\infty} x \cdot f(x) \cdot dx.
\]
\end{definition}

\begin{definition}
The \textit{conditional expectation} is the expected value of a random variable with respect to a conditional probability distribution.  This can be several things.  For example, let $X$ be a random variable, and $A$ be an event whose probability is not 0, then the conditional expectation is written as $E(X | A)$ and is understood as the expected value of the random variable $X$ given the event $A$.  If $Y$ is another random variable, then the conditional expectation $E(X|Y=y)$ is understood as the expected value of $X$ given the value $Y=y$ and this is a function of $y$.  The conditional expectation of $X$ given a random variable $Y$, denoted $E(X|Y)$ is also a random variable and is a function of $Y$.
\end{definition}

\begin{theorem}
For a discrete random variable $X$ and an event $A$ whose probability is not 0, then the \textit{conditional expectation} of $X$ given $A$ is defined as
\begin{eqnarray*}
	E(X|A) & = & \sum_x x \cdot P(x|A) \\
	& = & \sum_x x \cdot \frac{P(x \cap A)}{P(A)}.
\end{eqnarray*}
\end{theorem}



\begin{theorem}
For any two discrete random variables $X$ and $Y$, one may define the \textit{conditional expectation} as
\[
	E(X|Y = y) = \sum_x x \cdot P(X = x | Y = y).
\]
\end{theorem}


\begin{theorem}
In general, the population variance of a finite population of size $n$ is given by
\[
	\sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2.
\]
\end{theorem}



\begin{definition}
The \textit{standard deviation} is a measure of the dispertion of a set of values.
\end{definition}


\begin{theorem}
The standard deviation of a random variable is the root-mean-square deviation of its values from their mean.  Let $\overline{x}$ denote the mean value of the sample space:
\[
	\overline{x} = \frac{x_1+x_2+\cdots+x_n}{n} = \frac{1}{n} \sum_{i=1}^n x_i.
\]
Then the standard deviation of a random variable can be calculated as follows:
\begin{eqnarray*}
	\sigma & = & \sqrt {\frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2} \\
	& = & \sqrt {\frac{1}{n} \left( \sum_{i=1}^n {x_i}^2 \right) - \overline{x}^2 } \\
	& = & \frac{1}{n} \sqrt {n \left( \sum_{i=1}^n {x_i}^2 \right) - \left( \sum_{i=1}^n x_i \right)^2 }.
\end{eqnarray*}
The standard deviation of a random variable can also be given in terms of its expected value:
\begin{eqnarray*}
	\sigma & = & \sqrt {E((X-E(X))^2)} \\
	& = & \sqrt { E(X^2) - (E(X))^2 }
\end{eqnarray*}
\end{theorem}


\end{document}

